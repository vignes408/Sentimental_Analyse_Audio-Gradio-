# -*- coding: utf-8 -*-
"""sentimental_analysis_audio.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Eh-vCVnBrc9Xe0phxM-yF5UdEYVjRRHT
"""

pip install gradio

pip install gradio==3.29.0

pip install git+https://github.com/openai/whisper.git

pip install whisper

pip install torch

pip install torchvision

pip install torchaudio

!pip install transformers

import gradio as gr
from gradio import interface
import whisper
from transformers import pipeline

model = whisper.load_model("base")
sentiment_analysis = pipeline("sentiment-analysis", framework="pt", model="SamLowe/roberta-base-go_emotions")

def analyze_sentiment(text):
    results = sentiment_analysis(text)
    sentiment_results = {result['label']: result['score'] for result in results}
    return sentiment_results

def get_sentiment_emoji(sentiment):
    # Define the emojis corresponding to each sentiment
    emoji_mapping = {
        "disappointment": "ğŸ˜",
        "sadness": "ğŸ˜¢",
        "annoyance": "ğŸ˜ ",
        "neutral": "ğŸ˜",
        "disapproval": "ğŸ‘",
        "realization": "ğŸ˜®",
        "nervousness": "ğŸ˜¬",
        "approval": "ğŸ‘",
        "joy": "ğŸ˜„",
        "anger": "ğŸ˜¡",
        "embarrassment": "ğŸ˜³",
        "caring": "ğŸ¤—",
        "remorse": "ğŸ˜”",
        "disgust": "ğŸ¤¢",
        "grief": "ğŸ˜¥",
        "confusion": "ğŸ˜•",
        "relief": "ğŸ˜Œ",
        "desire": "ğŸ˜",
        "admiration": "ğŸ˜Œ",
        "optimism": "ğŸ˜Š",
        "fear": "ğŸ˜¨",
        "love": "â¤ï¸",
        "excitement": "ğŸ‰",
        "curiosity": "ğŸ¤”",
        "amusement": "ğŸ˜„",
        "surprise": "ğŸ˜²",
        "gratitude": "ğŸ™",
        "pride": "ğŸ¦"
    }
    return emoji_mapping.get(sentiment, "")

def display_sentiment_results(sentiment_results, option):
    sentiment_text = ""
    for sentiment, score in sentiment_results.items():
        emoji = get_sentiment_emoji(sentiment)
        if option == "Sentiment Only":
            sentiment_text += f"{sentiment} {emoji}\n"
        elif option == "Sentiment + Score":
            sentiment_text += f"{sentiment} {emoji}: {score}\n"
    return sentiment_text

def inference(audio, sentiment_option):
    audio = whisper.load_audio(audio)
    audio = whisper.pad_or_trim(audio)

    mel = whisper.log_mel_spectrogram(audio).to(model.device)

    _, probs = model.detect_language(mel)
    lang = max(probs, key=probs.get)

    options = whisper.DecodingOptions(fp16=False)
    result = whisper.decode(model, mel, options)

    sentiment_results = analyze_sentiment(result.text)
    sentiment_output = display_sentiment_results(sentiment_results, sentiment_option)

    return lang.upper(), result.text, sentiment_output

title = """<h1 align="center">Sentimental Analysis for the incoming Audio calls</h1>""" """<h2 align = "center">Welcome to Tech Evolution Sentimental Analyser</h2>"""
image_path = "/content/banner-image.jpg"
description = """<h2>HELLO EVERYONE</h2><h2>Our Motto is to provide the Best Service between the client and the customerğŸ¤œğŸ¤›</h2><br><h3>Welcome to our Page Here we provide the best serviceğŸ¤ for the Sentimental Analysis Using the Audio of the Incoming calls between the clients and the Customers.<br> and we try to identify the emotional tone of them and try to improve the Customer Satisfaction and the Customer Feedback<br></h3>
"""

custom_css = """
#banner-image {
    display: block;
    margin-left: auto;
    margin-right: auto;
}
#chat-message {
    font-size: 14px;
    min-height: 300px;
}
#body{
  background-image: url('/content/drive/MyDrive/bg.jpg');
  background-size: cover;
  background-repeat: no-repeat;
  background-attachment: fixed;
  background-position: center;
}
"""


block = gr.Blocks(css=custom_css)

with block:
    gr.HTML(title)
    with gr.Row():
        with gr.Column():
            gr.Image(image_path, elem_id="banner-image", show_label=False)
        with gr.Column():
            gr.HTML(description)

    with gr.Group():
        with gr.Box():
            audio = gr.Audio(
                label="Input Audio",
                show_label=False,
                source="microphone",
                type="filepath"
            )

            sentiment_option = gr.Radio(
                choices=["Sentiment Only", "Sentiment + Score"],
                label="Select an option",
                default="Sentiment Only"
            )

            btn = gr.Button("Transcribe")

        lang_str = gr.Textbox(label="Language")

        text = gr.Textbox(label="Transcription")

        sentiment_output = gr.Textbox(label="Sentiment Analysis Results", output=True)

        btn.click(inference, inputs=[audio, sentiment_option], outputs=[lang_str, text, sentiment_output])


        gr.HTML('''
        <div class="footer">
            <p>Model by Tech Evolution</p>
        </div>
        ''')

block.launch(share=True)

from google.colab import drive
drive.mount('/content/drive')